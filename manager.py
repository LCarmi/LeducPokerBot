import time

from game import *
from orderFile import *


class Manager:

    # Construct the Manager given the file path
    def __init__(self, file_path: str):

        node_lines, infoset_lines = text_order_by_history_length(file_path)
        self.originalGame = Game()
        self.originalGame.parse_game(node_lines, infoset_lines)
        self.abstractedGame = Game()
        self.abstractedGame.parse_game(node_lines, infoset_lines)
        self.information_set_mapping = {}

    def create_abstraction(self):
        self.information_set_mapping = self.abstractedGame.abstract_yourself()

    def map_strategies(self):
        for key in self.information_set_mapping:
            infoset_to_copy = self.information_set_mapping.get(key)
            infoset_to_update = self.originalGame.get_infoset_from_name(key)
            infoset_to_update.update_actions(infoset_to_copy)


    def write_result(self) -> str:
        infosets = self.originalGame.information_sets
        result = ""
        for infoset in infosets:
            result = result + str(infoset) + '\n'

        return result

    def create_virtual_game(self, game:Game, p:int, depth: int) -> Game:
        epsilon = 0.1
        res = game.find_nodes_at_depth_with_reach_probability(p,depth)
        l = len(res)

        actions = [str(i) for i in range(l)]
        probabilities = []
        children = []
        todo_list = []
        new_history_dictionary = {}
        new_information_sets = []
        for node,probability in res:
            children.append(node)
            probabilities.append(probability
                                 #* (1.0-epsilon) + epsilon/l #try more exploration
                                 )
        todo_list.extend(children)
        while todo_list:
            n = todo_list.pop()
            try:
                i = game.history_dictionary[n.name]
                new_history_dictionary[n.name] = i
                new_information_sets.append(i)
            except:
                pass

            todo_list.extend(n.children)

        new_root = ChanceNode("Virtual_root", actions, probabilities, children)
        ret = Game()
        ret.root_node = new_root
        ret.information_sets = new_information_sets
        ret.history_dictionary = new_history_dictionary
        return ret



    # def create_limited_depth_game(self, p,d):
    #     find_roots(self.originalGame.root_node, p ,d, 0, 1)
    #     for each found node, create subgame
    #     # create new root node, using subgames generated by found child and probabilities
    #
    # def find_roots(self, node:Node, p ,d, cur_depth, cur_prob) -> [(Node, double)]:
    #
    #     if cur_depth<d:
    #         # compute probabilities of reaching node
    #         for c in node.children:
    #             limit_tree(c, p, d, cur_depth+1, cur_prob * #probability of choosing child based on strategy/chance HINT: use infosets)
    #
    #
    # def create_subgame(self, node:Node, p): # used on each node at d level
    #         for each child in node.children:
    #             new_node = remove_player(child, p)
    #
    #         substitute new_child in children

def self_generative_refinement(manager):
    print("Self generative refinement")

    def mini_refinement(manager, player, adversary, depth):
        n_refinements = 10
        n_iter = 50
        d = 40
        virtual_game = manager.create_virtual_game(manager.originalGame, player, depth)
        # create infosets on chance/terminal nodes too
        # Leaf nodes as children

        # CFR on Leaf nodes ~ CFR on values of infosets
        # adversary_response on Leaf Nodes ~ pass dversary response on child
        if virtual_game.root_node.children:
            virtual_game.setup_masks(player, adversary)

            virtual_game.compute_masks(player, False) ##set max value here
            virtual_game.mask_yourself()
            virtual_game.solve_subgame(player, n_iter, d)
            virtual_game.update_infoset_from_subgame()
            for i in range(n_refinements):
                virtual_game.restore_masks()
                virtual_game.adversary_response(player, adversary)
                virtual_game.compute_masks(player, True) ## compute infoset value
                virtual_game.mask_yourself()
                virtual_game.solve_subgame(player, n_iter, d)
                virtual_game.update_infoset_from_subgame()

            virtual_game.restore_masks()

            virtual_game.clean_masks()


    depth = 1
    while depth < 8: #TODO: something better maybe
        mini_refinement(manager, 1, 2, depth)
        mini_refinement(manager, 2, 1, depth)
        print("Refined level: " + str(depth))
        depth += 1

def bias_refinement(manager):
    print("Bias refinement")

    def mini_refinement(manager, player, adversary, depth):
        n_iter = 4000
        d = 3000
        profiles = [
            (1, 1, 10),
            (1, 10, 1),
            (10, 1, 1),
        ]
        virtual_game = manager.create_virtual_game(manager.originalGame, player, depth)

        if virtual_game.root_node.children:
            virtual_game.setup_masks(player, adversary) # create infosets on chance/terminal nodes too
            virtual_game.compute_bias_masks(profiles, player, adversary)

            virtual_game.mask_yourself()

            for i in virtual_game.information_sets:
                i.prepare_for_CFR()
            virtual_game.CFR_plus_optimize()
            virtual_game.update_infoset_from_subgame()
            virtual_game.restore_masks()

            virtual_game.clean_masks()

    depth = 1
    while depth < 8: #TODO: something better maybe
        mini_refinement(manager, 1, 2, depth)
        mini_refinement(manager, 2, 1, depth)
        print("Refined level: " + str(depth))
        depth += 1



def CFR_refinement(manager):
    print("CFR refinement")
    n_iter = 1000
    d = 500
    player = 1
    other_player = 2
    #Do the subgame for each level
    depth=1
    virtual_game1 = manager.create_virtual_game(manager.originalGame, player, depth)
    virtual_game2 = manager.create_virtual_game(manager.originalGame, other_player, depth)
    while virtual_game1.root_node.children != [] or virtual_game2.root_node.children != []:
        if virtual_game1.root_node.children:
            virtual_game1.solve_subgame(player, n_iter, d)
            virtual_game1.update_infoset_from_subgame()

        if virtual_game2.root_node.children:
            virtual_game2.solve_subgame(other_player, n_iter, d)
            virtual_game2.update_infoset_from_subgame()

        print("Refined level: " + str(depth))

        depth += 1
        virtual_game1 = manager.create_virtual_game(manager.originalGame, player, depth)
        virtual_game2 = manager.create_virtual_game(manager.originalGame, other_player, depth)

def ingame_refinement(manager, depth):
    print("InGame refinement")

    profiles = [
        (1, 1, 10),
        (1, 10, 1),
        (10, 1, 1),
    ] # check, raise, fold

    def single_refinement(player, adversary, depth):
        manager.originalGame.init_leaves(depth)
        manager.originalGame.setup_masks_ingame(adversary)  # create infosets on chance/terminal nodes too
        manager.originalGame.compute_bias_masks_ingame(profiles, adversary)
        manager.originalGame.mask_yourself_ingame()
        for i in manager.originalGame.information_sets:
            i.prepare_for_CFR()
        manager.originalGame.CFR_plus_optimize()
        manager.originalGame.restore_masks_ingame()
        manager.originalGame.clean_masks_ingame()
        manager.originalGame.map_strategies_until_depth(manager.originalGame.root_node,player,depth)

    print("P1 refinement")
    single_refinement(1,2,depth)
    print("P2 refinement")
    single_refinement(2,1,depth)


# if __name__ == '__main__':
#     basename = "Leduc_A_4_cards"
#     file_path = "./Examples/Leduc_A.txt"
#     Game.d = 10000  # number of regret explorations without strategy update
#     Game.total_iterations = 20000  # number of iteration to do
#     Game.n_groups = 4  # number of card groups
#
#     manager = Manager(file_path)
#     print("Game loaded!")
#
#     t_start = time.time()
#     manager.create_abstraction()
#     print("Abstraction ended!")
#     t_end_abstraction = time.time()
#
#     manager.abstractedGame.find_optimal_strategy()
#     #manager.originalGame.find_optimal_strategy()
#     print("Blue print strategy done in abstract game!")
#
#     manager.map_strategies()
#     print("Blue print mapped on the real game")
#     t_blueprint = time.time()
#
#     # Initialize the final strategy
#     for infoset in manager.originalGame.information_sets:
#         infoset.final_strategy = infoset.get_average_strategy()
#
#     exploitability_blueprint = manager.originalGame.exploitability_Luca()
#     expected_val_blueprint = manager.originalGame.root_node.expected_value(manager.originalGame.history_dictionary)
#     res_blue = manager.write_result()
#
#     Game.d = 2000  # number of regret explorations without strategy update
#     Game.total_iterations = 4000  # number of iteration to do
#
#     t_start_ref = time.time()
#     #print(manager.write_result())
#     #res = manager.write_result()
#     print("Refine strategy start")
#
#     #bias_refinement(manager)
#     #self_generative_refinement(manager)
#     #CFR_refinement(manager)
#     ingame_refinement(manager, 6)
#     t_end_ref = time.time()
#     print("Refine strategy done")
#     #print(manager.write_result())
#
#     exploitability_ref = manager.originalGame.exploitability_Luca()
#     expected_val_ref = manager.originalGame.root_node.expected_value(manager.originalGame.history_dictionary)
#     res_ref = manager.write_result()
#
#     s = ""
#     s += "Time for Abstraction: {}\n".format(t_end_abstraction-t_start)
#     s += "Time for Blueprint: {}\n".format(t_blueprint-t_end_abstraction)
#     s += "Time for Refinement: {}\n".format(t_end_ref-t_start_ref)
#     s += "Expected Value Blueprint: {}\n".format(expected_val_blueprint)
#     s += "Exploitability Blueprint: {}\n".format(exploitability_blueprint)
#     s += "Expected Value Refinement: {}\n".format(expected_val_ref)
#     s += "Exploitability Refinement: {}\n".format(exploitability_ref)
#
#     print(s)
#
#     file_path_output = "./Examples/Output_" + basename + "_blueprint.txt"
#     f = open(file_path_output, "w+")
#     f.write(res_blue)
#     f.close()
#
#     file_path_output = "./Examples/Output_" + basename + "_refined.txt"
#     f = open(file_path_output, "w+")
#     f.write(res_ref)
#     f.close()
#
#     file_path_output = "./Examples/Output_" + basename + "_log.txt"
#     f = open(file_path_output, "w+")
#     f.write(s)
#     f.close()
#
#     print("Write finished")


if __name__ == '__main__':
    basename = "Leduc_A_5_cards"
    file_path = "./Examples/Leduc_A.txt"
    Game.d = 10000  # number of regret explorations without strategy update
    Game.total_iterations = 20000  # number of iteration to do
    Game.n_groups = 5  # number of card groups

    manager = Manager(file_path)
    print("Game loaded!")

    t_end_abstraction = time.time()

    manager.originalGame.find_optimal_strategy()

    t_blueprint = time.time()

    # Initialize the final strategy
    for infoset in manager.originalGame.information_sets:
        infoset.final_strategy = infoset.get_average_strategy()

    exploitability_res = manager.originalGame.exploitability_Luca()
    expected_val_res = manager.originalGame.root_node.expected_value(manager.originalGame.history_dictionary)
    res = manager.write_result()

    s = ""
    s += "Time for Solution: {}\n".format(t_blueprint-t_end_abstraction)
    s += "Expected Value Blueprint: {}\n".format(expected_val_res)
    s += "Exploitability Blueprint: {}\n".format(exploitability_res)

    print(s)

    file_path_output = "./Examples/Output_" + basename + "_res.txt"
    f = open(file_path_output, "w+")
    f.write(res)
    f.close()

    file_path_output = "./Examples/Output_" + basename + "_log.txt"
    f = open(file_path_output, "w+")
    f.write(s)
    f.close()

    print("Write finished")
